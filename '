import os
import argparse
import numpy as np
import pandas as pd

from torch.utils.data import DataLoader

from transformers import EncoderDecoderModel

from config import *
from dataset import *

def main(args):
    tokenizer = load_tokenizer()
    
    ##model warm_starting
    model = EncoderDecoderModel.from_encoder_decoder_pretrained(
        'bert-base-uncased', 'bert-base-uncased'
    )

    model.config.decoder_start_token_id = tokenizer.cls_token_id
    model.config.eos_token_id = tokenizer.sep_token_id
    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.vocab_size = model.config.decoder.vocab_size

    model.config.max_length = 142
    model.config.min_length = 56
    model.config.no_repeat_ngram_size = 3
    model.config.early_stopping = True
    model.config.length_penalty = 2.0
    model.config.num_beams = 4

    inputs = tokenizer(args.text, padding="max_length", truncation=True, max_length=SEQ_MAX_LEN, return_tensors="pt")
    input_ids = inputs.input_ids.to(device)
    attention_mask = inputs.attention_mask.to(device)

    outputs = model.generate(input_ids, attention_mask=attention_mask)
    output_str = tokenizer.decode(outputs, skip_special_tokens=True)

    print(input_text)
    print(output_text)

    del model
    del tokenizer

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--train_file', type=str, help='path for training csv')
    parser.add_argument('--valid_file', type=str, help='full csv file with labels')
    parser.add_argument('--xcol', type=str, help='inputs')
    parser.add_argument('--ycol', type=str, help='labels')
    parser.add_argument('--nrows', type=int, default=10000, help='no of rows used for training')

    options = parser.parse_args()
    main(options)

 

